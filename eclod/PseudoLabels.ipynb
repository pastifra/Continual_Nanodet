{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T16:23:29.226237478Z",
     "start_time": "2024-02-19T16:23:27.721982515Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 9\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "import yaml\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import numpy as np\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "from pytorch_lightning.accelerators import find_usable_cuda_devices\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nanodet.data.collate import naive_collate\n",
    "from nanodet.data.dataset import build_dataset\n",
    "from nanodet.evaluator import build_evaluator\n",
    "from nanodet.trainer.task import TrainingTask\n",
    "from nanodet.trainer.pseudo_lab_task import PseudoLabelTrainingTask\n",
    "from nanodet.trainer.dist_task import DistTrainingTask\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from nanodet.util import (\n",
    "    NanoDetLightningLogger,\n",
    "    cfg,\n",
    "    convert_old_model,\n",
    "    env_utils,\n",
    "    load_config,\n",
    "    load_model_weight,\n",
    "    mkdir,\n",
    ")\n",
    "\n",
    "#Set logger and seed\n",
    "logger = NanoDetLightningLogger('test')\n",
    "pl.seed_everything(9)\n",
    "\n",
    "#Function to create the task configuration file required for training\n",
    "def create_exp_cfg(yml_path, task):\n",
    "    all_names = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "    #Load the YAML file\n",
    "    with open(yml_path, 'r') as file:\n",
    "        temp_cfg = yaml.safe_load(file)\n",
    "    #Save dir of the model\n",
    "    temp_cfg['save_dir'] = 'models/task' + str(task)\n",
    "    #If base task, training and testing classes are the same\n",
    "    if task == 0:\n",
    "        temp_cfg['data']['train']['class_names'] = all_names[:15]\n",
    "        temp_cfg['data']['val']['class_names'] = all_names[:15]\n",
    "        temp_cfg['model']['arch']['head']['num_classes'] = 20 #15\n",
    "        #temp_cfg['model']['arch']['aux_head']['num_classes'] = 20 #15\n",
    "    #Else, training only on task specific class, and testing on all classes\n",
    "    else:\n",
    "        temp_cfg['data']['train']['class_names'] = [all_names[14+task]]\n",
    "        temp_cfg['data']['val']['class_names'] = all_names[:15+task]\n",
    "        temp_cfg['model']['arch']['head']['num_classes'] = 20#15+task\n",
    "        #temp_cfg['model']['arch']['aux_head']['num_classes'] = 20#15+task\n",
    "        temp_cfg['schedule']['load_model'] = 'models/task' + str(task-1) + '/model_last.ckpt'\n",
    "        \n",
    "    temp_cfg_name = 'cfg/task' + str(task) + '.yml'\n",
    "    print(temp_cfg_name)\n",
    "    #Save the new configuration file\n",
    "    with open(temp_cfg_name, 'w') as file:\n",
    "        yaml.safe_dump(temp_cfg, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T16:23:36.331323677Z",
     "start_time": "2024-02-19T16:23:30.333719337Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[1m\u001B[35m[NanoDet]\u001B[0m\u001B[34m[02-19 17:23:30]\u001B[0m\u001B[32mINFO:\u001B[0m\u001B[97mStarting task1\u001B[0m\n",
      "\u001B[1m\u001B[35m[NanoDet]\u001B[0m\u001B[34m[02-19 17:23:30]\u001B[0m\u001B[32mINFO:\u001B[0m\u001B[97mStarting task1\u001B[0m\n",
      "\u001B[1m\u001B[35m[NanoDet]\u001B[0m\u001B[34m[02-19 17:23:30]\u001B[0m\u001B[32mINFO:\u001B[0m\u001B[97mSetting up data...\u001B[0m\n",
      "\u001B[1m\u001B[35m[NanoDet]\u001B[0m\u001B[34m[02-19 17:23:30]\u001B[0m\u001B[32mINFO:\u001B[0m\u001B[97mSetting up data...\u001B[0m\n",
      "WARNING:root:WARNING! Keeping only annotations of these categories [{'supercategory': 'pottedplant', 'id': 16, 'name': 'pottedplant'}]! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg/task1.yml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:WARNING! Keeping only annotations of these categories [{'supercategory': 'aeroplane', 'id': 1, 'name': 'aeroplane'}, {'supercategory': 'bicycle', 'id': 2, 'name': 'bicycle'}, {'supercategory': 'bird', 'id': 3, 'name': 'bird'}, {'supercategory': 'boat', 'id': 4, 'name': 'boat'}, {'supercategory': 'bottle', 'id': 5, 'name': 'bottle'}, {'supercategory': 'bus', 'id': 6, 'name': 'bus'}, {'supercategory': 'car', 'id': 7, 'name': 'car'}, {'supercategory': 'cat', 'id': 8, 'name': 'cat'}, {'supercategory': 'chair', 'id': 9, 'name': 'chair'}, {'supercategory': 'cow', 'id': 10, 'name': 'cow'}, {'supercategory': 'diningtable', 'id': 11, 'name': 'diningtable'}, {'supercategory': 'dog', 'id': 12, 'name': 'dog'}, {'supercategory': 'horse', 'id': 13, 'name': 'horse'}, {'supercategory': 'motorbike', 'id': 14, 'name': 'motorbike'}, {'supercategory': 'person', 'id': 15, 'name': 'person'}, {'supercategory': 'pottedplant', 'id': 16, 'name': 'pottedplant'}]! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 273 xml files and 625 boxes\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[1m\u001B[35m[NanoDet]\u001B[0m\u001B[34m[02-19 17:23:30]\u001B[0m\u001B[32mINFO:\u001B[0m\u001B[97mCreating models\u001B[0m\n",
      "\u001B[1m\u001B[35m[NanoDet]\u001B[0m\u001B[34m[02-19 17:23:30]\u001B[0m\u001B[32mINFO:\u001B[0m\u001B[97mCreating models\u001B[0m\n",
      "INFO:NanoDet:Creating models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 4530 xml files and 13606 boxes\n",
      "creating index...\n",
      "index created!\n",
      "model size is  1.0x\n",
      "init weights...\n",
      "=> loading pretrained model https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth\n",
      "Finish initialize NanoDet-Plus Head.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[1m\u001B[35m[NanoDet]\u001B[0m\u001B[34m[02-19 17:23:32]\u001B[0m\u001B[32mINFO:\u001B[0m\u001B[97mLoaded model weight from models/task0/model_last.ckpt\u001B[0m\n",
      "\u001B[1m\u001B[35m[NanoDet]\u001B[0m\u001B[34m[02-19 17:23:32]\u001B[0m\u001B[32mINFO:\u001B[0m\u001B[97mLoaded model weight from models/task0/model_last.ckpt\u001B[0m\n",
      "INFO:NanoDet:Loaded model weight from models/task0/model_last.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name  | Type        | Params\n",
      "--------------------------------------\n",
      "0 | model | NanoDetPlus | 1.2 M \n",
      "--------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.699     Total estimated model params size (MB)\n",
      "/home/pasti/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "/home/pasti/anaconda3/envs/nanodet/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 94\u001B[0m\n\u001B[1;32m     77\u001B[0m     env_utils\u001B[38;5;241m.\u001B[39mset_multi_processing(distributed\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     79\u001B[0m trainer \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mTrainer(\n\u001B[1;32m     80\u001B[0m     default_root_dir\u001B[38;5;241m=\u001B[39mcfg\u001B[38;5;241m.\u001B[39msave_dir,\n\u001B[1;32m     81\u001B[0m     max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     92\u001B[0m     precision\u001B[38;5;241m=\u001B[39mprecision,\n\u001B[1;32m     93\u001B[0m )\n\u001B[0;32m---> 94\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mTrainTask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_resume_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:608\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    606\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_unwrap_optimized(model)\n\u001B[1;32m    607\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m_lightning_module \u001B[38;5;241m=\u001B[39m model\n\u001B[0;32m--> 608\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    609\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    610\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     36\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 38\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[1;32m     41\u001B[0m     trainer\u001B[38;5;241m.\u001B[39m_call_teardown_hook()\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:650\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    643\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m ckpt_path \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresume_from_checkpoint\n\u001B[1;32m    644\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_set_ckpt_path(\n\u001B[1;32m    645\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[1;32m    646\u001B[0m     ckpt_path,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    647\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    648\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    649\u001B[0m )\n\u001B[0;32m--> 650\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    652\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[1;32m    653\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39mrestore_training_state()\n\u001B[1;32m   1110\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39mresume_end()\n\u001B[0;32m-> 1112\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1114\u001B[0m log\u001B[38;5;241m.\u001B[39mdetail(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1115\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_teardown()\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1191\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1189\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredicting:\n\u001B[1;32m   1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_predict()\n\u001B[0;32m-> 1191\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1214\u001B[0m, in \u001B[0;36mTrainer._run_train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1211\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mtrainer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\n\u001B[1;32m   1213\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m-> 1214\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 199\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:267\u001B[0m, in \u001B[0;36mFitLoop.advance\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher\u001B[38;5;241m.\u001B[39msetup(dataloader, batch_to_device\u001B[38;5;241m=\u001B[39mbatch_to_device)\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_epoch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 267\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_fetcher\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 199\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:213\u001B[0m, in \u001B[0;36mTrainingEpochLoop.advance\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    210\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_started()\n\u001B[1;32m    212\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_batch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 213\u001B[0m         batch_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_processed()\n\u001B[1;32m    217\u001B[0m \u001B[38;5;66;03m# update non-plateau LR schedulers\u001B[39;00m\n\u001B[1;32m    218\u001B[0m \u001B[38;5;66;03m# update epoch-interval ones only when we are at the end of training epoch\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 199\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88\u001B[0m, in \u001B[0;36mTrainingBatchLoop.advance\u001B[0;34m(self, kwargs)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mlightning_module\u001B[38;5;241m.\u001B[39mautomatic_optimization:\n\u001B[1;32m     85\u001B[0m     optimizers \u001B[38;5;241m=\u001B[39m _get_active_optimizers(\n\u001B[1;32m     86\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39moptimizers, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39moptimizer_frequencies, kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch_idx\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     87\u001B[0m     )\n\u001B[0;32m---> 88\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     90\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanual_loop\u001B[38;5;241m.\u001B[39mrun(kwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 199\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:202\u001B[0m, in \u001B[0;36mOptimizerLoop.advance\u001B[0;34m(self, optimizers, kwargs)\u001B[0m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madvance\u001B[39m(\u001B[38;5;28mself\u001B[39m, optimizers: List[Tuple[\u001B[38;5;28mint\u001B[39m, Optimizer]], kwargs: OrderedDict) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    200\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_kwargs(kwargs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer_idx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_hiddens)\n\u001B[0;32m--> 202\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_optimization\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptim_progress\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_position\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result\u001B[38;5;241m.\u001B[39mloss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    204\u001B[0m         \u001B[38;5;66;03m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001B[39;00m\n\u001B[1;32m    205\u001B[0m         \u001B[38;5;66;03m# would be skipped otherwise\u001B[39;00m\n\u001B[1;32m    206\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_outputs[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer_idx] \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39masdict()\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:249\u001B[0m, in \u001B[0;36mOptimizerLoop._run_optimization\u001B[0;34m(self, kwargs, optimizer)\u001B[0m\n\u001B[1;32m    241\u001B[0m         closure()\n\u001B[1;32m    243\u001B[0m \u001B[38;5;66;03m# ------------------------------\u001B[39;00m\n\u001B[1;32m    244\u001B[0m \u001B[38;5;66;03m# BACKWARD PASS\u001B[39;00m\n\u001B[1;32m    245\u001B[0m \u001B[38;5;66;03m# ------------------------------\u001B[39;00m\n\u001B[1;32m    246\u001B[0m \u001B[38;5;66;03m# gradient update with accumulated gradients\u001B[39;00m\n\u001B[1;32m    247\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    248\u001B[0m     \u001B[38;5;66;03m# the `batch_idx` is optional with inter-batch parallelism\u001B[39;00m\n\u001B[0;32m--> 249\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopt_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbatch_idx\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    251\u001B[0m result \u001B[38;5;241m=\u001B[39m closure\u001B[38;5;241m.\u001B[39mconsume_result()\n\u001B[1;32m    253\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result\u001B[38;5;241m.\u001B[39mloss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    254\u001B[0m     \u001B[38;5;66;03m# if no result, user decided to skip optimization\u001B[39;00m\n\u001B[1;32m    255\u001B[0m     \u001B[38;5;66;03m# otherwise update running loss + reset accumulated loss\u001B[39;00m\n\u001B[1;32m    256\u001B[0m     \u001B[38;5;66;03m# TODO: find proper way to handle updating running loss\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:370\u001B[0m, in \u001B[0;36mOptimizerLoop._optimizer_step\u001B[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001B[0m\n\u001B[1;32m    362\u001B[0m     rank_zero_deprecation(\n\u001B[1;32m    363\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe NVIDIA/apex AMP implementation has been deprecated upstream. Consequently, its integration inside\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    364\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m PyTorch Lightning has been deprecated in v1.9.0 and will be removed in v2.0.0.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    367\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m return True.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    368\u001B[0m     )\n\u001B[1;32m    369\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124musing_native_amp\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprecision_plugin, MixedPrecisionPlugin)\n\u001B[0;32m--> 370\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_lightning_module_hook\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    371\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moptimizer_step\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    372\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcurrent_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    373\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    374\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    375\u001B[0m \u001B[43m    \u001B[49m\u001B[43mopt_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    376\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_step_and_backward_closure\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    377\u001B[0m \u001B[43m    \u001B[49m\u001B[43mon_tpu\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTPUAccelerator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    378\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m    379\u001B[0m \u001B[43m    \u001B[49m\u001B[43musing_lbfgs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_lbfgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    380\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    382\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m should_accumulate:\n\u001B[1;32m    383\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptim_progress\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep\u001B[38;5;241m.\u001B[39mincrement_completed()\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1356\u001B[0m, in \u001B[0;36mTrainer._call_lightning_module_hook\u001B[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1353\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m hook_name\n\u001B[1;32m   1355\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LightningModule]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpl_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1356\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m   1359\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[0;32m~/PycharmProjects/nanodet_cl/nanodet/trainer/pseudo_lab_task.py:304\u001B[0m, in \u001B[0;36mPseudoLabelTrainingTask.optimizer_step\u001B[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_lbfgs)\u001B[0m\n\u001B[1;32m    301\u001B[0m         pg[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m pg[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minitial_lr\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m*\u001B[39m k\n\u001B[1;32m    303\u001B[0m \u001B[38;5;66;03m# update params\u001B[39;00m\n\u001B[0;32m--> 304\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclosure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer_closure\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    305\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:169\u001B[0m, in \u001B[0;36mLightningOptimizer.step\u001B[0;34m(self, closure, **kwargs)\u001B[0m\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MisconfigurationException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_strategy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 169\u001B[0m step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_on_after_step()\n\u001B[1;32m    173\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m step_output\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:234\u001B[0m, in \u001B[0;36mStrategy.optimizer_step\u001B[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001B[39;00m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, pl\u001B[38;5;241m.\u001B[39mLightningModule)\n\u001B[0;32m--> 234\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprecision_plugin\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    235\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_idx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mopt_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    236\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:119\u001B[0m, in \u001B[0;36mPrecisionPlugin.optimizer_step\u001B[0;34m(self, optimizer, model, optimizer_idx, closure, **kwargs)\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001B[39;00m\n\u001B[1;32m    118\u001B[0m closure \u001B[38;5;241m=\u001B[39m partial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001B[0;32m--> 119\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclosure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001B[0m, in \u001B[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     66\u001B[0m instance\u001B[38;5;241m.\u001B[39m_step_count \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     67\u001B[0m wrapped \u001B[38;5;241m=\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__get__\u001B[39m(instance, \u001B[38;5;28mcls\u001B[39m)\n\u001B[0;32m---> 68\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    138\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[0;32m--> 140\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m     obj\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[0;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/torch/optim/adamw.py:120\u001B[0m, in \u001B[0;36mAdamW.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m closure \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39menable_grad():\n\u001B[0;32m--> 120\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m group \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_groups:\n\u001B[1;32m    123\u001B[0m     params_with_grad \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:105\u001B[0m, in \u001B[0;36mPrecisionPlugin._wrap_closure\u001B[0;34m(self, model, optimizer, optimizer_idx, closure)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wrap_closure\u001B[39m(\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     94\u001B[0m     model: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.LightningModule\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     97\u001B[0m     closure: Callable[[], Any],\n\u001B[1;32m     98\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m     99\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001B[39;00m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;124;03m    ``on_before_optimizer_step`` hook is called.\u001B[39;00m\n\u001B[1;32m    101\u001B[0m \n\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001B[39;00m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;124;03m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001B[39;00m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 105\u001B[0m     closure_result \u001B[38;5;241m=\u001B[39m \u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    106\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_closure(model, optimizer, optimizer_idx)\n\u001B[1;32m    107\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m closure_result\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:149\u001B[0m, in \u001B[0;36mClosure.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[Tensor]:\n\u001B[0;32m--> 149\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    150\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\u001B[38;5;241m.\u001B[39mloss\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:135\u001B[0m, in \u001B[0;36mClosure.closure\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mclosure\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ClosureResult:\n\u001B[0;32m--> 135\u001B[0m     step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_step_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m step_output\u001B[38;5;241m.\u001B[39mclosure_loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    138\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwarning_cache\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:419\u001B[0m, in \u001B[0;36mOptimizerLoop._training_step\u001B[0;34m(self, kwargs)\u001B[0m\n\u001B[1;32m    410\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Performs the actual train step with the tied hooks.\u001B[39;00m\n\u001B[1;32m    411\u001B[0m \n\u001B[1;32m    412\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    416\u001B[0m \u001B[38;5;124;03m    A ``ClosureResult`` containing the training step output.\u001B[39;00m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    418\u001B[0m \u001B[38;5;66;03m# manually capture logged metrics\u001B[39;00m\n\u001B[0;32m--> 419\u001B[0m training_step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_strategy_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtraining_step\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    420\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mpost_training_step()\n\u001B[1;32m    422\u001B[0m model_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39m_call_lightning_module_hook(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining_step_end\u001B[39m\u001B[38;5;124m\"\u001B[39m, training_step_output)\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1494\u001B[0m, in \u001B[0;36mTrainer._call_strategy_hook\u001B[0;34m(self, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1491\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m   1493\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1494\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:378\u001B[0m, in \u001B[0;36mStrategy.training_step\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    376\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprecision_plugin\u001B[38;5;241m.\u001B[39mtrain_step_context():\n\u001B[1;32m    377\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, TrainingStep)\n\u001B[0;32m--> 378\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/nanodet_cl/nanodet/trainer/pseudo_lab_task.py:99\u001B[0m, in \u001B[0;36mPseudoLabelTrainingTask.training_step\u001B[0;34m(self, batch, batch_idx)\u001B[0m\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28mid\u001B[39m \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;66;03m### NANODET LOSS ###\u001B[39;00m\n\u001B[0;32m---> 99\u001B[0m loss, loss_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhead\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhead_out\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;66;03m### LOGGING ###\u001B[39;00m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcfg\u001B[38;5;241m.\u001B[39mlog\u001B[38;5;241m.\u001B[39minterval \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/nanodet_cl/nanodet/model/head/nanodet_plus_head.py:214\u001B[0m, in \u001B[0;36mNanoDetPlusHead.loss\u001B[0;34m(self, preds, gt_meta, aux_preds)\u001B[0m\n\u001B[1;32m    203\u001B[0m     batch_assign_res \u001B[38;5;241m=\u001B[39m multi_apply(\n\u001B[1;32m    204\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_assign_single_img,\n\u001B[1;32m    205\u001B[0m         aux_cls_preds\u001B[38;5;241m.\u001B[39mdetach(),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    210\u001B[0m         gt_bboxes_ignore,\n\u001B[1;32m    211\u001B[0m     )\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    213\u001B[0m     \u001B[38;5;66;03m# use self prediction to assign\u001B[39;00m\n\u001B[0;32m--> 214\u001B[0m     batch_assign_res \u001B[38;5;241m=\u001B[39m \u001B[43mmulti_apply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    215\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_assign_single_img\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    216\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcls_preds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    217\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcenter_priors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    218\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecoded_bboxes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    219\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgt_bboxes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    220\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgt_labels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    221\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgt_bboxes_ignore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    222\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    224\u001B[0m loss, loss_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_loss_from_assign(\n\u001B[1;32m    225\u001B[0m     cls_preds, reg_preds, decoded_bboxes, batch_assign_res\n\u001B[1;32m    226\u001B[0m )\n\u001B[1;32m    228\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m aux_preds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/PycharmProjects/nanodet_cl/nanodet/util/misc.py:24\u001B[0m, in \u001B[0;36mmulti_apply\u001B[0;34m(func, *args, **kwargs)\u001B[0m\n\u001B[1;32m     22\u001B[0m pfunc \u001B[38;5;241m=\u001B[39m partial(func, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;28;01melse\u001B[39;00m func\n\u001B[1;32m     23\u001B[0m map_results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(pfunc, \u001B[38;5;241m*\u001B[39margs)\n\u001B[0;32m---> 24\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmap_results\u001B[49m\u001B[43m)\u001B[49m))\n",
      "File \u001B[0;32m~/anaconda3/envs/nanodet/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[0;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/nanodet_cl/nanodet/model/head/nanodet_plus_head.py:331\u001B[0m, in \u001B[0;36mNanoDetPlusHead.target_assign_single_img\u001B[0;34m(self, cls_preds, center_priors, decoded_bboxes, gt_bboxes, gt_labels, gt_bboxes_ignore)\u001B[0m\n\u001B[1;32m    328\u001B[0m     gt_bboxes_ignore \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(gt_bboxes_ignore)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m    329\u001B[0m     gt_bboxes_ignore \u001B[38;5;241m=\u001B[39m gt_bboxes_ignore\u001B[38;5;241m.\u001B[39mto(decoded_bboxes\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[0;32m--> 331\u001B[0m assign_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massigner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massign\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    332\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcls_preds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    333\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcenter_priors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    334\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecoded_bboxes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    335\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgt_bboxes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    336\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgt_labels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    337\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgt_bboxes_ignore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    338\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    339\u001B[0m pos_inds, neg_inds, pos_gt_bboxes, pos_assigned_gt_inds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msample(\n\u001B[1;32m    340\u001B[0m     assign_result, gt_bboxes\n\u001B[1;32m    341\u001B[0m )\n\u001B[1;32m    343\u001B[0m num_priors \u001B[38;5;241m=\u001B[39m center_priors\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/nanodet_cl/nanodet/model/head/assigner/dsl_assigner.py:61\u001B[0m, in \u001B[0;36mDynamicSoftLabelAssigner.assign\u001B[0;34m(self, pred_scores, priors, decoded_bboxes, gt_bboxes, gt_labels, gt_bboxes_ignore)\u001B[0m\n\u001B[1;32m     58\u001B[0m assigned_gt_inds \u001B[38;5;241m=\u001B[39m decoded_bboxes\u001B[38;5;241m.\u001B[39mnew_full((num_bboxes,), \u001B[38;5;241m0\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\n\u001B[1;32m     60\u001B[0m prior_center \u001B[38;5;241m=\u001B[39m priors[:, :\u001B[38;5;241m2\u001B[39m]\n\u001B[0;32m---> 61\u001B[0m lt_ \u001B[38;5;241m=\u001B[39m prior_center[:, \u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m-\u001B[39m \u001B[43mgt_bboxes\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m     62\u001B[0m rb_ \u001B[38;5;241m=\u001B[39m gt_bboxes[:, \u001B[38;5;241m2\u001B[39m:] \u001B[38;5;241m-\u001B[39m prior_center[:, \u001B[38;5;28;01mNone\u001B[39;00m]\n\u001B[1;32m     64\u001B[0m deltas \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([lt_, rb_], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mIndexError\u001B[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "###LEARNING STREAM###\n",
    "#task 0: train on first 15 classes, test on 15 classes\n",
    "#task 1: train on class n°16, test on 16 classes\n",
    "#task 2: train on class n°17, test on 17 classes\n",
    "#task 3: train on class n°18, test on 18 classes\n",
    "#task 4: train on class n°19, test on 19 classes\n",
    "#task 5: train on class n°20, test on 20 classes\n",
    "#torch.set_printoptions(profile=\"full\")\n",
    "#opt_epochs = [60, 80, 40, 60 ,40]\n",
    "for task in range (1, 6):\n",
    "    logger = NanoDetLightningLogger('run_logs/task'+str(task))\n",
    "    logger.info(\"Starting task\" + str(task))\n",
    "    logger.info(\"Setting up data...\")\n",
    "    #Create the task configuration file based on the task number and load the configuration\n",
    "    create_exp_cfg('cfg/VOC.yml', task)\n",
    "    load_config(cfg, 'cfg/task' + str(task) + '.yml')\n",
    "    #Build datasets and dataloaders based on the task configuration file\n",
    "    train_dataset = build_dataset(cfg.data.train, \"train\")\n",
    "    #If task is not 0, create the replay dataset using the buffer\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg.device.batchsize_per_gpu,\n",
    "        shuffle=True,\n",
    "        num_workers=cfg.device.workers_per_gpu,\n",
    "        pin_memory=True,\n",
    "        collate_fn=naive_collate,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_dataset = build_dataset(cfg.data.val, \"test\")\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=cfg.device.batchsize_per_gpu,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.device.workers_per_gpu,\n",
    "        pin_memory=True,\n",
    "        collate_fn=naive_collate,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    evaluator = build_evaluator(cfg.evaluator, val_dataset)\n",
    "    \n",
    "    #Create the model based on the task configuration file\n",
    "    logger.info(\"Creating models\")\n",
    "    if task == 0:\n",
    "        TrainTask = TrainingTask(cfg, evaluator)\n",
    "    else:\n",
    "        TrainTask = PseudoLabelTrainingTask(cfg, evaluator)\n",
    "        #Load the model weights if task is not 0\n",
    "        if \"load_model\" in cfg.schedule:\n",
    "            ckpt = torch.load(cfg.schedule.load_model)\n",
    "            load_model_weight(TrainTask.model, ckpt, logger)\n",
    "            logger.info(\"Loaded model weight from {}\".format(cfg.schedule.load_model))\n",
    "    \n",
    "    model_resume_path = (\n",
    "        os.path.join(cfg.save_dir, \"model_last.ckpt\")\n",
    "        if \"resume\" in cfg.schedule\n",
    "        else None\n",
    "    )\n",
    "    #Set the device to GPU if available\n",
    "    if cfg.device.gpu_ids == -1:\n",
    "        logger.info(\"Using CPU training\")\n",
    "        accelerator, devices, strategy, precision = (\n",
    "            \"cpu\",\n",
    "            None,\n",
    "            None,\n",
    "            cfg.device.precision,\n",
    "        )\n",
    "    else:\n",
    "        accelerator, devices, strategy, precision = (\n",
    "            \"gpu\",\n",
    "            cfg.device.gpu_ids,\n",
    "            None,\n",
    "            cfg.device.precision,\n",
    "        )\n",
    "\n",
    "    if devices and len(devices) > 1:\n",
    "        strategy = \"ddp\"\n",
    "        env_utils.set_multi_processing(distributed=True)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=cfg.save_dir,\n",
    "        max_epochs=100,\n",
    "        check_val_every_n_epoch=10,\n",
    "        accelerator=accelerator,\n",
    "        devices=[2],\n",
    "        log_every_n_steps=cfg.log.interval,\n",
    "        num_sanity_val_steps=0,\n",
    "        callbacks=[TQDMProgressBar(refresh_rate=0)],# TrainTask.early_stop_callback],\n",
    "        logger=logger,\n",
    "        benchmark=cfg.get(\"cudnn_benchmark\", True),\n",
    "        gradient_clip_val=cfg.get(\"grad_clip\", 0.0),\n",
    "        strategy=strategy,\n",
    "        precision=precision,\n",
    "    )\n",
    "    trainer.fit(TrainTask, train_dataloader, val_dataloader, ckpt_path=model_resume_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DEBUGGING"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[1m\u001B[35m[NanoDet]\u001B[0m\u001B[34m[02-19 17:09:06]\u001B[0m\u001B[32mINFO:\u001B[0m\u001B[97mStarting task1\u001B[0m\n",
      "\u001B[1m\u001B[35m[NanoDet]\u001B[0m\u001B[34m[02-19 17:09:06]\u001B[0m\u001B[32mINFO:\u001B[0m\u001B[97mStarting task1\u001B[0m\n",
      "\u001B[1m\u001B[35m[NanoDet]\u001B[0m\u001B[34m[02-19 17:09:06]\u001B[0m\u001B[32mINFO:\u001B[0m\u001B[97mStarting task1\u001B[0m\n",
      "INFO:NanoDet:Starting task1\n",
      "\u001B[1m\u001B[35m[NanoDet]\u001B[0m\u001B[34m[02-19 17:09:06]\u001B[0m\u001B[32mINFO:\u001B[0m\u001B[97mSetting up data...\u001B[0m\n",
      "\u001B[1m\u001B[35m[NanoDet]\u001B[0m\u001B[34m[02-19 17:09:06]\u001B[0m\u001B[32mINFO:\u001B[0m\u001B[97mSetting up data...\u001B[0m\n",
      "\u001B[1m\u001B[35m[NanoDet]\u001B[0m\u001B[34m[02-19 17:09:06]\u001B[0m\u001B[32mINFO:\u001B[0m\u001B[97mSetting up data...\u001B[0m\n",
      "INFO:NanoDet:Setting up data...\n",
      "WARNING:root:WARNING! Keeping only annotations of these categories [{'supercategory': 'pottedplant', 'id': 16, 'name': 'pottedplant'}]! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg/task1.yml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:WARNING! Keeping only annotations of these categories [{'supercategory': 'aeroplane', 'id': 1, 'name': 'aeroplane'}, {'supercategory': 'bicycle', 'id': 2, 'name': 'bicycle'}, {'supercategory': 'bird', 'id': 3, 'name': 'bird'}, {'supercategory': 'boat', 'id': 4, 'name': 'boat'}, {'supercategory': 'bottle', 'id': 5, 'name': 'bottle'}, {'supercategory': 'bus', 'id': 6, 'name': 'bus'}, {'supercategory': 'car', 'id': 7, 'name': 'car'}, {'supercategory': 'cat', 'id': 8, 'name': 'cat'}, {'supercategory': 'chair', 'id': 9, 'name': 'chair'}, {'supercategory': 'cow', 'id': 10, 'name': 'cow'}, {'supercategory': 'diningtable', 'id': 11, 'name': 'diningtable'}, {'supercategory': 'dog', 'id': 12, 'name': 'dog'}, {'supercategory': 'horse', 'id': 13, 'name': 'horse'}, {'supercategory': 'motorbike', 'id': 14, 'name': 'motorbike'}, {'supercategory': 'person', 'id': 15, 'name': 'person'}, {'supercategory': 'pottedplant', 'id': 16, 'name': 'pottedplant'}]! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 273 xml files and 625 boxes\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[1m\u001B[35m[NanoDet]\u001B[0m\u001B[34m[02-19 17:09:06]\u001B[0m\u001B[32mINFO:\u001B[0m\u001B[97mCreating models\u001B[0m\n",
      "\u001B[1m\u001B[35m[NanoDet]\u001B[0m\u001B[34m[02-19 17:09:06]\u001B[0m\u001B[32mINFO:\u001B[0m\u001B[97mCreating models\u001B[0m\n",
      "\u001B[1m\u001B[35m[NanoDet]\u001B[0m\u001B[34m[02-19 17:09:06]\u001B[0m\u001B[32mINFO:\u001B[0m\u001B[97mCreating models\u001B[0m\n",
      "INFO:NanoDet:Creating models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 4530 xml files and 13606 boxes\n",
      "creating index...\n",
      "index created!\n",
      "model size is  1.0x\n",
      "init weights...\n",
      "=> loading pretrained model https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth\n",
      "Finish initialize NanoDet-Plus Head.\n",
      "model size is  1.0x\n",
      "init weights...\n",
      "=> loading pretrained model https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth\n",
      "Finish initialize NanoDet-Plus Head.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[1m\u001B[35m[NanoDet]\u001B[0m\u001B[34m[02-19 17:09:07]\u001B[0m\u001B[32mINFO:\u001B[0m\u001B[97mLoaded model weight from models/task0/model_last.ckpt\u001B[0m\n",
      "\u001B[1m\u001B[35m[NanoDet]\u001B[0m\u001B[34m[02-19 17:09:07]\u001B[0m\u001B[32mINFO:\u001B[0m\u001B[97mLoaded model weight from models/task0/model_last.ckpt\u001B[0m\n",
      "\u001B[1m\u001B[35m[NanoDet]\u001B[0m\u001B[34m[02-19 17:09:07]\u001B[0m\u001B[32mINFO:\u001B[0m\u001B[97mLoaded model weight from models/task0/model_last.ckpt\u001B[0m\n",
      "INFO:NanoDet:Loaded model weight from models/task0/model_last.ckpt\n"
     ]
    }
   ],
   "source": [
    "task = 1\n",
    "logger = NanoDetLightningLogger('run_logs/task'+str(task))\n",
    "logger.info(\"Starting task\" + str(task))\n",
    "logger.info(\"Setting up data...\")\n",
    "#Create the task configuration file based on the task number and load the configuration\n",
    "create_exp_cfg('cfg/VOC.yml', task)\n",
    "load_config(cfg, 'cfg/task' + str(task) + '.yml')\n",
    "#Build datasets and dataloaders based on the task configuration file\n",
    "train_dataset = build_dataset(cfg.data.train, \"train\")\n",
    "#If task is not 0, create the replay dataset using the buffer\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=cfg.device.workers_per_gpu,\n",
    "    pin_memory=True,\n",
    "    collate_fn=naive_collate,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_dataset = build_dataset(cfg.data.val, \"test\")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.device.workers_per_gpu,\n",
    "    pin_memory=True,\n",
    "    collate_fn=naive_collate,\n",
    "    drop_last=False,\n",
    ")\n",
    "evaluator = build_evaluator(cfg.evaluator, val_dataset)\n",
    "\n",
    "#Create the model based on the task configuration file\n",
    "logger.info(\"Creating models\")\n",
    "if task == 0:\n",
    "    TrainTask = TrainingTask(cfg, evaluator)\n",
    "else:\n",
    "    TrainTask = DistTrainingTask(cfg, evaluator)\n",
    "    #Load the model weights if task is not 0\n",
    "    if \"load_model\" in cfg.schedule:\n",
    "        ckpt = torch.load(cfg.schedule.load_model)\n",
    "        load_model_weight(TrainTask.model, ckpt, logger)\n",
    "        load_model_weight(TrainTask.teacher, ckpt, logger)\n",
    "        logger.info(\"Loaded model weight from {}\".format(cfg.schedule.load_model))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T16:09:07.269648749Z",
     "start_time": "2024-02-19T16:09:06.424887298Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for batch in val_dataloader:\n",
    "    batch = TrainTask._preprocess_batch_input(batch)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T16:21:57.926479736Z",
     "start_time": "2024-02-19T16:21:55.987373031Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[140.72072, 185.58559, 171.42342, 211.17117]], dtype=float32), array([[158.72, 101.12, 231.68, 186.88],\n",
      "       [ 97.92,  98.56, 132.48, 160.  ],\n",
      "       [ 73.6 ,  90.24,  90.24, 120.96],\n",
      "       [ 91.52,  85.76, 108.8 , 112.64]], dtype=float32), array([[ 28.05988  ,  39.47006  , 151.14072  , 218.67065  ],\n",
      "       [198.33234  , 140.23055  , 213.       , 198.26347  ],\n",
      "       [191.9551   , 144.69461  , 200.88324  , 188.05988  ],\n",
      "       [177.92516  , 147.88324  , 189.40419  , 181.0449   ],\n",
      "       [169.63474  , 146.60779  , 180.47604  , 187.42215  ],\n",
      "       [183.66467  , 147.24551  , 195.78143  , 154.89821  ],\n",
      "       [  1.2754492, 151.07185  ,  29.973053 , 161.91318  ],\n",
      "       [  0.6377246, 153.62276  ,  16.58084  , 168.29042  ]],\n",
      "      dtype=float32), array([[202.08981  ,  98.847305 , 309.86526  , 157.51796  ],\n",
      "       [117.27245  ,  87.36826  , 215.48204  , 149.86526  ],\n",
      "       [ 31.17964  ,  88.643715 , 108.344315 , 151.14072  ],\n",
      "       [  2.4820359,  85.45509  ,  38.832336 , 137.7485   ],\n",
      "       [183.59581  ,  87.36826  , 199.53893  , 147.9521   ]],\n",
      "      dtype=float32), array([[ 39.04,  78.72, 222.72, 144.64]], dtype=float32), array([[  0.93731344,  97.73731   , 283.28955   , 214.        ],\n",
      "       [155.52835   , 122.01194   , 312.67462   , 189.72537   ],\n",
      "       [ 77.59403   ,  84.32239   , 202.1612    , 121.37313   ]],\n",
      "      dtype=float32), array([[ 28.35503 ,  40.260357, 161.91716 , 145.70415 ]], dtype=float32), array([[110.08759 ,  41.593674, 206.71289 , 219.48662 ],\n",
      "       [ 67.85401 ,  83.82725 , 247.02676 , 243.80292 ]], dtype=float32), array([[  7.036036,  85.16216 , 132.40541 , 274.49548 ],\n",
      "       [ 62.045044,  54.45946 , 186.13513 , 199.65765 ],\n",
      "       [ 45.414413, 130.57658 , 197.64865 , 274.49548 ],\n",
      "       [110.65766 ,  81.96397 , 213.      , 218.20721 ]], dtype=float32), array([[ 19.18919 ,  59.576576, 257.77478 , 283.45044 ]], dtype=float32), array([[ 64.054054,  45.414413, 274.49548 , 213.      ]], dtype=float32), array([[131.2 , 175.36, 270.08, 226.56]], dtype=float32), array([[127.36,  56.32, 236.16, 240.  ]], dtype=float32), array([[  1.3843416,   0.6370107, 212.87189  , 178.36299  ]],\n",
      "      dtype=float32), array([[115.93976 ,  12.771085, 168.93976 , 136.6506  ],\n",
      "       [ 23.987951,  42.144577, 231.51807 , 184.54218 ]], dtype=float32), array([[ 16.64, 161.92,  30.08, 170.88],\n",
      "       [ 36.48, 117.76, 277.76, 195.84]], dtype=float32), array([[113.,  51., 221., 201.],\n",
      "       [154.,  25., 196., 120.]], dtype=float32), array([[173.4382  , 177.85393 , 225.08708 , 319.41013 ],\n",
      "       [  8.926967,  13.980337, 190.6545  , 319.41013 ]], dtype=float32), array([[206.08, 145.28, 305.28, 211.84],\n",
      "       [  3.84, 129.28,  47.36, 156.8 ],\n",
      "       [ 91.52, 121.6 , 125.44, 149.76],\n",
      "       [ 88.96, 111.36, 114.56, 132.48],\n",
      "       [ 58.88, 114.56,  85.12, 139.52],\n",
      "       [  8.32, 114.56,  25.6 , 127.36],\n",
      "       [130.56, 107.52, 153.6 , 141.44],\n",
      "       [129.92,  93.44, 147.84, 103.04],\n",
      "       [204.8 , 123.52, 217.6 , 140.8 ]], dtype=float32), array([[245.12, 108.8 , 273.92, 152.96],\n",
      "       [156.16, 118.4 , 211.2 , 149.76],\n",
      "       [ 32.64, 163.84,  96.  , 197.12]], dtype=float32), array([[  0.64,  45.44, 186.24, 320.  ],\n",
      "       [ 20.48,  62.72,  99.84, 199.04],\n",
      "       [ 93.44, 102.4 , 100.48, 134.4 ],\n",
      "       [ 99.84, 106.24, 115.84, 154.88],\n",
      "       [ 36.48, 113.92, 117.76, 208.64],\n",
      "       [ 33.28, 128.  , 215.04, 320.  ],\n",
      "       [212.48, 117.76, 240.  , 176.  ],\n",
      "       [186.88, 174.72, 239.36, 320.  ]], dtype=float32), array([[191.89189 ,  48.7027  , 213.      , 149.12613 ],\n",
      "       [118.333336,  46.144146, 188.05405 , 304.55856 ]], dtype=float32), array([[ 27.594595,  47.972973, 199.01802 , 158.63063 ]], dtype=float32), array([[ 35.90991 ,  10.234234, 310.95496 , 213.      ]], dtype=float32), array([[ 32.507553,  50.359516, 145.97583 , 133.86707 ],\n",
      "       [143.42598 ,  51.63444 , 265.18127 , 150.44109 ],\n",
      "       [156.81268 ,  24.861027, 265.18127 , 145.97885 ],\n",
      "       [ 54.18127 ,  22.94864 , 146.6133  , 130.67976 ]], dtype=float32), array([[ 33.99099  ,  30.063063 , 293.04504  , 204.68468  ],\n",
      "       [  1.3693694,   5.117117 ,  78.12613  , 112.57658  ]],\n",
      "      dtype=float32), array([[ 64.693695,  86.35135 ,  94.75676 , 111.936935],\n",
      "       [116.5045  ,  94.666664, 143.36937 , 110.01802 ],\n",
      "       [ 42.306305,  83.15315 , 191.34235 , 165.02702 ]], dtype=float32), array([[  9.187166,  73.4893  ,  57.75401 , 238.36096 ],\n",
      "       [129.3262  ,  77.96257 , 297.39304 , 222.38503 ],\n",
      "       [ 66.0615  ,  82.43583 , 172.78075 , 171.26204 ],\n",
      "       [  9.826203, 162.95454 , 237.32353 , 239.      ]], dtype=float32), array([[ 38.4 ,  61.44, 113.92, 240.  ],\n",
      "       [102.4 ,  86.4 , 175.36, 240.  ],\n",
      "       [206.72,  78.72, 269.44, 240.  ],\n",
      "       [285.44,  58.88, 320.  , 240.  ],\n",
      "       [266.88, 111.36, 288.  , 145.92]], dtype=float32), array([[131.84,  64.64, 233.6 , 224.64],\n",
      "       [ 46.72,   0.64, 266.88, 240.  ]], dtype=float32), array([[ 70.4 ,  48.  , 179.84, 283.52],\n",
      "       [  7.04, 144.64, 240.  , 320.  ]], dtype=float32), array([[202.24,   8.96, 320.  , 135.04]], dtype=float32), array([[ 60.216217 ,  35.18018  , 152.96396  , 184.85585  ],\n",
      "       [110.74775  ,   0.6396396, 319.9099   , 191.25226  ]],\n",
      "      dtype=float32), array([[ 55.0991 ,  36.45946, 180.46848, 213.     ],\n",
      "       [161.27928,  70.36036, 302.     , 213.     ]], dtype=float32), array([[194.54054  ,  40.936935 , 319.9099   , 213.       ],\n",
      "       [  0.7297297,  56.28829  , 108.82883  , 198.92793  ],\n",
      "       [ 19.918919 , 165.02702  ,  39.10811  , 213.       ],\n",
      "       [  8.405405 , 174.62163  ,  26.315315 , 211.72072  ],\n",
      "       [261.7027   , 159.27026  , 279.6126   , 191.89189  ]],\n",
      "      dtype=float32), array([[114.56,  17.92, 238.72, 202.88],\n",
      "       [117.76,  41.6 , 297.6 , 227.84]], dtype=float32), array([[147.84,  60.8 , 270.08, 240.  ],\n",
      "       [  3.2 ,  72.96, 155.52, 240.  ],\n",
      "       [ 78.08,  74.88, 108.16, 120.32],\n",
      "       [  0.64,  56.96,  82.56, 204.8 ],\n",
      "       [222.72, 103.68, 263.68, 177.92],\n",
      "       [257.92,  90.24, 301.44, 147.84],\n",
      "       [280.96,  92.8 , 300.16, 117.12]], dtype=float32), array([[  1.92,   8.96, 197.76, 240.  ],\n",
      "       [156.16,   0.64, 297.6 , 240.  ],\n",
      "       [ 98.56,  97.28, 193.28, 240.  ]], dtype=float32), array([[ 41.6 , 111.36, 192.64, 157.44],\n",
      "       [188.8 , 117.12, 247.68, 155.52],\n",
      "       [251.52, 106.88, 291.2 , 153.6 ],\n",
      "       [162.56, 106.88, 181.76, 157.44],\n",
      "       [ 67.2 , 115.84,  80.  , 167.04],\n",
      "       [ 91.52, 107.52, 106.88, 135.68],\n",
      "       [ 64.64, 113.92,  90.88, 160.64],\n",
      "       [165.76,  97.28, 247.68, 116.48]], dtype=float32), array([[ 75.64706,  64.54278, 301.22726, 239.     ]], dtype=float32), array([[170.21687 ,  33.843372, 299.84338 , 212.      ],\n",
      "       [ 22.710844,  99.614456, 158.72289 , 212.      ]], dtype=float32), array([[240.62773  , 111.34306  , 319.97568  , 245.08272  ],\n",
      "       [ 32.019466 , 128.62044  , 162.55962  , 263.       ],\n",
      "       [151.68126  , 106.86375  , 229.74939  , 254.04137  ],\n",
      "       [158.08029  ,   2.5596106, 218.23114  , 216.927    ],\n",
      "       [243.18735  ,  17.917274 , 319.97568  , 211.16788  ],\n",
      "       [ 34.579075 ,  16.63747  , 174.71776  , 231.64478  ]],\n",
      "      dtype=float32), array([[ 44.864864,  44.135136, 252.10811 , 186.13513 ]], dtype=float32), array([[  1.3693694,   7.036036 , 316.07208  , 212.36037  ],\n",
      "       [  0.7297297,   2.5585585, 110.74775  ,  83.79279  ]],\n",
      "      dtype=float32), array([[ 63.36,  77.44, 222.72, 170.88]], dtype=float32), array([[ 60.16,  32.  , 175.36, 183.68],\n",
      "       [149.12,  37.12, 254.72, 161.28],\n",
      "       [243.2 ,  39.04, 309.12, 149.76]], dtype=float32), array([[  0.64,  32.64, 240.  , 320.  ]], dtype=float32), array([[ 31.432432 ,   0.6396396, 303.27927  , 213.       ]],\n",
      "      dtype=float32), array([[  0.64,  19.84, 280.96, 227.2 ]], dtype=float32), array([[116.      ,  62.666668, 201.33333 , 319.33334 ]], dtype=float32), array([[119.68,  96.  , 184.96, 152.32],\n",
      "       [ 64.  ,  88.32,  88.96, 152.32],\n",
      "       [183.04,  89.6 , 206.72, 152.32],\n",
      "       [274.56,  84.48, 298.88, 152.32],\n",
      "       [  9.6 ,  97.92,  95.36, 149.76],\n",
      "       [202.88, 100.48, 278.4 , 149.12]], dtype=float32), array([[ 39.26567 ,  14.053731, 319.7015  , 214.      ]], dtype=float32), array([[ 62.72,  40.96, 256.64, 225.28]], dtype=float32), array([[ 33.10955  ,   1.9129213, 278.60114  , 227.       ],\n",
      "       [  3.1404495,  67.58989  , 224.40169  , 227.       ],\n",
      "       [287.52808  ,  79.06741  , 312.39606  , 124.33989  ]],\n",
      "      dtype=float32), array([[108.189186,  56.28829 , 168.31532 , 138.8018  ]], dtype=float32), array([[112.86121 ,  10.192171, 126.87544 ,  53.508896],\n",
      "       [103.94306 ,  21.021353, 123.05338 ,  58.60498 ]], dtype=float32), array([[ 80.  ,  85.76, 237.44, 151.04]], dtype=float32), array([[ 61.495495,  23.027027, 289.84683 , 170.14415 ]], dtype=float32), array([[ 53.76, 165.76, 136.32, 229.12],\n",
      "       [ 55.68,   0.64, 320.  , 206.08]], dtype=float32), array([[ 85.8018 ,  71.63964, 182.38739, 133.68468]], dtype=float32), array([[213.76,  66.56, 300.16, 160.  ],\n",
      "       [108.16,  19.84, 213.12, 117.76],\n",
      "       [  1.28,   1.28, 119.04, 127.36],\n",
      "       [ 75.52,  74.88, 147.84, 156.8 ],\n",
      "       [146.56,  98.56, 220.16, 172.16],\n",
      "       [174.72, 156.16, 256.64, 238.08],\n",
      "       [ 91.52, 133.76, 174.72, 229.76],\n",
      "       [ 17.28, 122.88,  97.28, 210.56]], dtype=float32), array([[  0.64,  51.84, 137.6 , 320.  ]], dtype=float32), array([[ 62.08,   9.6 , 259.2 , 239.36]], dtype=float32), array([[290.56, 122.24, 316.8 , 138.24],\n",
      "       [247.04, 106.24, 261.76, 120.96],\n",
      "       [169.6 , 115.84, 205.44, 149.76],\n",
      "       [175.36,  96.  , 193.28, 116.48],\n",
      "       [161.28, 106.88, 174.72, 134.4 ],\n",
      "       [136.32, 112.64, 163.2 , 152.32],\n",
      "       [108.8 , 102.4 , 131.84, 142.72],\n",
      "       [ 78.08,  94.72, 107.52, 127.36],\n",
      "       [ 72.96,  69.12, 109.44, 104.32]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "gt_boxes = batch[\"gt_bboxes\"]\n",
    "print(gt_boxes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T16:20:10.241873147Z",
     "start_time": "2024-02-19T16:20:10.203916580Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "img = batch[\"img\"]\n",
    "stud_feat = TrainTask.model.backbone(img)\n",
    "stud_fpn_feat = TrainTask.model.fpn(stud_feat)\n",
    "head_out = TrainTask.model.head(stud_fpn_feat)\n",
    "dets = TrainTask.model.head.post_process(head_out, batch)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T16:22:03.722327462Z",
     "start_time": "2024-02-19T16:22:02.755868811Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "id = 0\n",
    "for img_id, img_dets in dets.items():\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for label, bboxes in img_dets.items():\n",
    "        for bbox in bboxes:\n",
    "            score = bbox[-1]\n",
    "            if score > 0.3:\n",
    "                x0, y0, x1, y1 = [int(i) for i in bbox[:4]]\n",
    "                boxes.append([x0, y0, x1, y1])\n",
    "                labels.append(label)\n",
    "    \n",
    "    batch[\"gt_bboxes\"][id] = np.append(batch[\"gt_bboxes\"][id],np.array(boxes,dtype=np.float32))\n",
    "    batch[\"gt_labels\"][id] = np.append(batch[\"gt_labels\"][id],np.array(labels))\n",
    "    id += 1\n",
    "print(len(batch[\"gt_bboxes\"]))\n",
    "        #batch[\"gt_bboxes\"].extend(np.array(all_box, dtype=np.float32))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T16:22:35.413800569Z",
     "start_time": "2024-02-19T16:22:35.402152342Z"
    }
   },
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 96, 16, 20])\n"
     ]
    }
   ],
   "source": [
    "print(teach_int_head_out[1].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T09:16:28.939355086Z",
     "start_time": "2024-02-19T09:16:28.911482590Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 116, 32, 40])\n",
      "torch.Size([1, 116, 32, 40])\n",
      "torch.Size([1, 232, 16, 20])\n",
      "torch.Size([1, 232, 16, 20])\n",
      "torch.Size([1, 464, 8, 10])\n",
      "torch.Size([1, 464, 8, 10])\n",
      "tensor(0., grad_fn=<MseLossBackward0>)\n",
      "tensor(0., grad_fn=<MseLossBackward0>)\n",
      "tensor(0., grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "stud_feat = TrainTask.model.backbone(img)\n",
    "lista = []\n",
    "lista1 = []\n",
    "for tensor in stud_feat:\n",
    "    tensor_prob = nn.functional.softmax(tensor, dim=1)\n",
    "    lista.append(tensor)\n",
    "    lista1.append(tensor_prob)\n",
    "    print(tensor_prob.shape)\n",
    "    print(tensor.shape)\n",
    "mse_loss = nn.MSELoss()\n",
    "for tensor1, tensor2 in zip(stud_feat,stud_feat):\n",
    "    loss = mse_loss(tensor1, tensor2)\n",
    "    print(loss)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T08:59:42.860739876Z",
     "start_time": "2024-02-19T08:59:42.784029288Z"
    }
   },
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\n    output = gfl_cls(feat)\\n    outputs.append(output.flatten(start_dim=2))\\noutputs = torch.cat(outputs, dim=2).permute(0, 2, 1)\\n'"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "outputs = []\n",
    "for feat, cls_convs in zip(\n",
    "    stud_fpn_feat,\n",
    "    TrainTask.model.head.cls_convs,\n",
    "):\n",
    "    for conv in cls_convs:\n",
    "        feat = conv(feat)\n",
    "    outputs.append(feat)\n",
    "    \n",
    "\n",
    "mse_list = []\n",
    "for tensor1, tensor2 in zip(outputs,outputs):\n",
    "    mse = F.mse_loss(tensor1, tensor2)\n",
    "    mse_list.append(mse.item())   \n",
    "print(mse_list)\n",
    "'''\n",
    "    output = gfl_cls(feat)\n",
    "    outputs.append(output.flatten(start_dim=2))\n",
    "outputs = torch.cat(outputs, dim=2).permute(0, 2, 1)\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T13:49:46.440965312Z",
     "start_time": "2024-02-15T13:49:46.425614135Z"
    }
   },
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DepthwiseConvModule(\n",
      "  (depthwise): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
      "  (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (dwnorm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pwnorm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "teacher_state_dict = TrainTask.teacher.head.state_dict()\n",
    "student_state_dict = TrainTask.model.head.state_dict()\n",
    "print(TrainTask.teacher.head.cls_convs[0][1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T11:56:11.434892746Z",
     "start_time": "2024-02-15T11:56:11.356309101Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "head = TrainTask.teacher.head"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T12:01:51.929766699Z",
     "start_time": "2024-02-15T12:01:51.888824451Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): DepthwiseConvModule(\n",
      "    (depthwise): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
      "    (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (dwnorm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (pwnorm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (1): DepthwiseConvModule(\n",
      "    (depthwise): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
      "    (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (dwnorm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (pwnorm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      ")\n",
      "ModuleList(\n",
      "  (0): DepthwiseConvModule(\n",
      "    (depthwise): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
      "    (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (dwnorm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (pwnorm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (1): DepthwiseConvModule(\n",
      "    (depthwise): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
      "    (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (dwnorm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (pwnorm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      ")\n",
      "ModuleList(\n",
      "  (0): DepthwiseConvModule(\n",
      "    (depthwise): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
      "    (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (dwnorm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (pwnorm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (1): DepthwiseConvModule(\n",
      "    (depthwise): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
      "    (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (dwnorm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (pwnorm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      ")\n",
      "ModuleList(\n",
      "  (0): DepthwiseConvModule(\n",
      "    (depthwise): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
      "    (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (dwnorm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (pwnorm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (1): DepthwiseConvModule(\n",
      "    (depthwise): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
      "    (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (dwnorm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (pwnorm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for conv in head.cls_convs:\n",
    "    print(conv)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T12:02:07.965283148Z",
     "start_time": "2024-02-15T12:02:07.910537954Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "##################\n",
    "# COPY HEAD TEST #\n",
    "##################\n",
    "# Get the state dict of both models\n",
    "teacher_state_dict = TrainTask.teacher.head.gfl_cls.state_dict()\n",
    "student_state_dict = TrainTask.model.head.gfl_cls.state_dict()\n",
    "# Update the specific layer weights in the student model with the teacher model weights\n",
    "for name, param in teacher_state_dict.items():\n",
    "    # Slice the weights tensor along the out_channels dimension from 0 to 15\n",
    "    student_state_dict[name][:17] = param[:17]\n",
    "# Load the updated state dict into the student model\n",
    "TrainTask.model.head.gfl_cls.load_state_dict(student_state_dict)\n",
    "trainer.save_checkpoint(\"models/task1/model_last.ckpt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T11:42:13.655651932Z",
     "start_time": "2024-02-15T11:42:13.631978953Z"
    }
   },
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanodet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
