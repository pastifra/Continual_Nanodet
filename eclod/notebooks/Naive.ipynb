{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T14:56:00.880789666Z",
     "start_time": "2024-02-22T14:56:00.870735139Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "import yaml\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "from pytorch_lightning.accelerators import find_usable_cuda_devices\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nanodet.data.collate import naive_collate\n",
    "from nanodet.data.dataset import build_dataset\n",
    "from nanodet.evaluator import build_evaluator\n",
    "from nanodet.trainer.task import TrainingTask\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from nanodet.util import (\n",
    "    NanoDetLightningLogger,\n",
    "    cfg,\n",
    "    convert_old_model,\n",
    "    env_utils,\n",
    "    load_config,\n",
    "    load_model_weight,\n",
    "    mkdir,\n",
    ")\n",
    "\n",
    "#Set logger and seed\n",
    "logger = NanoDetLightningLogger('test')\n",
    "pl.seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T10:16:49.248537174Z",
     "start_time": "2024-02-07T10:16:49.230460755Z"
    }
   },
   "outputs": [],
   "source": [
    "#Function to create the task configuration file required for training\n",
    "def create_exp_cfg(yml_path, task):\n",
    "    all_names = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "    #Load the YAML file\n",
    "    with open(yml_path, 'r') as file:\n",
    "        temp_cfg = yaml.safe_load(file)\n",
    "    #Save dir of the model\n",
    "    temp_cfg['save_dir'] = 'models/task' + str(task)\n",
    "    #If base task, training and testing classes are the same\n",
    "    if task == 0:\n",
    "        temp_cfg['data']['train']['class_names'] = all_names[:15]\n",
    "        temp_cfg['data']['val']['class_names'] = all_names[:15]\n",
    "        temp_cfg['model']['arch']['head']['num_classes'] = 20 #15\n",
    "        #temp_cfg['model']['arch']['aux_head']['num_classes'] = 20 #15\n",
    "    #Else, training only on task specific class, and testing on all classes\n",
    "    else:\n",
    "        temp_cfg['data']['train']['class_names'] = [all_names[14+task]]\n",
    "        temp_cfg['data']['val']['class_names'] = all_names[:15+task]\n",
    "        temp_cfg['model']['arch']['head']['num_classes'] = 20#15+task\n",
    "        #temp_cfg['model']['arch']['aux_head']['num_classes'] = 20#15+task\n",
    "        temp_cfg['schedule']['load_model'] = 'models/task' + str(task-1) + '/model_last.ckpt'\n",
    "        \n",
    "    temp_cfg_name = 'cfg/task' + str(task) + '.yml'\n",
    "    print(temp_cfg_name)\n",
    "    #Save the new configuration file\n",
    "    with open(temp_cfg_name, 'w') as file:\n",
    "        yaml.safe_dump(temp_cfg, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning stream\n",
    "#task 0: train on first 15 classes, test on 15 classes\n",
    "#task 1: train on class n°16, test on 16 classes\n",
    "#task 2: train on class n°17, test on 17 classes\n",
    "#task 3: train on class n°18, test on 18 classes\n",
    "#task 4: train on class n°19, test on 19 classes\n",
    "#task 5: train on class n°20, test on 20 classes\n",
    "for task in range (1, 6):\n",
    "    logger = NanoDetLightningLogger('run_logs/task'+str(task))\n",
    "    logger.info(\"Starting task\" + str(task))\n",
    "    logger.info(\"Setting up data...\")\n",
    "    #Create the task configuration file based on the task number and load the configuration\n",
    "    create_exp_cfg('cfg/VOC.yml', task)\n",
    "    load_config(cfg, 'cfg/task' + str(task) + '.yml')\n",
    "    #Build datasets and dataloaders based on the task configuration file\n",
    "    train_dataset = build_dataset(cfg.data.train, \"train\")\n",
    "    val_dataset = build_dataset(cfg.data.val, \"test\")\n",
    "    evaluator = build_evaluator(cfg.evaluator, val_dataset)\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg.device.batchsize_per_gpu,\n",
    "        shuffle=True,\n",
    "        num_workers=cfg.device.workers_per_gpu,\n",
    "        pin_memory=True,\n",
    "        collate_fn=naive_collate,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=cfg.device.batchsize_per_gpu,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.device.workers_per_gpu,\n",
    "        pin_memory=True,\n",
    "        collate_fn=naive_collate,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    #Create the model based on the task configuration file\n",
    "    logger.info(\"Creating model...\")\n",
    "    task = TrainingTask(cfg, evaluator)\n",
    "    #Load the model weights if task is not 0\n",
    "    if \"load_model\" in cfg.schedule:\n",
    "        ckpt = torch.load(cfg.schedule.load_model)\n",
    "        if \"pytorch-lightning_version\" not in ckpt:\n",
    "            warnings.warn(\n",
    "                \"Warning! Old .pth checkpoint is deprecated. \"\n",
    "                \"Convert the checkpoint with tools/convert_old_checkpoint.py \"\n",
    "            )\n",
    "            ckpt = convert_old_model(ckpt)\n",
    "        load_model_weight(task.model, ckpt, logger)\n",
    "        logger.info(\"Loaded model weight from {}\".format(cfg.schedule.load_model))\n",
    "    model_resume_path = (\n",
    "        os.path.join(cfg.save_dir, \"model_last.ckpt\")\n",
    "        if \"resume\" in cfg.schedule\n",
    "        else None\n",
    "    )\n",
    "    #Set the device to GPU if available\n",
    "    if cfg.device.gpu_ids == -1:\n",
    "        logger.info(\"Using CPU training\")\n",
    "        accelerator, devices, strategy, precision = (\n",
    "            \"cpu\",\n",
    "            None,\n",
    "            None,\n",
    "            cfg.device.precision,\n",
    "        )\n",
    "    else:\n",
    "        accelerator, devices, strategy, precision = (\n",
    "            \"gpu\",\n",
    "            cfg.device.gpu_ids,\n",
    "            None,\n",
    "            cfg.device.precision,\n",
    "        )\n",
    "\n",
    "    if devices and len(devices) > 1:\n",
    "        strategy = \"ddp\"\n",
    "        env_utils.set_multi_processing(distributed=True)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=cfg.save_dir,\n",
    "        max_epochs=cfg.schedule.total_epochs,\n",
    "        check_val_every_n_epoch=cfg.schedule.val_intervals,\n",
    "        accelerator=accelerator,\n",
    "        devices=[2],\n",
    "        log_every_n_steps=cfg.log.interval,\n",
    "        num_sanity_val_steps=0,\n",
    "        callbacks=[TQDMProgressBar(refresh_rate=0)],\n",
    "        logger=logger,\n",
    "        benchmark=cfg.get(\"cudnn_benchmark\", True),\n",
    "        gradient_clip_val=cfg.get(\"grad_clip\", 0.0),\n",
    "        strategy=strategy,\n",
    "        precision=precision,\n",
    "    )\n",
    "    trainer.fit(task, train_dataloader, val_dataloader, ckpt_path=model_resume_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T15:00:25.769061566Z",
     "start_time": "2024-02-22T15:00:25.087706484Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Learning stream\n",
    "#task 0: train on first 15 classes, test on 15 classes\n",
    "#task 1: train on class n°16, test on 16 classes\n",
    "#task 2: train on class n°17, test on 17 classes\n",
    "#task 3: train on class n°18, test on 18 classes\n",
    "#task 4: train on class n°19, test on 19 classes\n",
    "#task 5: train on class n°20, test on 20 classes\n",
    "\n",
    "\n",
    "#Create the task configuration file based on the task number and load the configuration\n",
    "load_config(cfg, 'cfg/VOCsingle.yml')\n",
    "#Build datasets and dataloaders based on the task configuration file\n",
    "train_dataset = build_dataset(cfg.data.train, \"train\")\n",
    "val_dataset = build_dataset(cfg.data.val, \"test\")\n",
    "evaluator = build_evaluator(cfg.evaluator, val_dataset)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=cfg.device.batchsize_per_gpu,\n",
    "    shuffle=True,\n",
    "    num_workers=cfg.device.workers_per_gpu,\n",
    "    pin_memory=True,\n",
    "    collate_fn=naive_collate,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=cfg.device.batchsize_per_gpu,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.device.workers_per_gpu,\n",
    "    pin_memory=True,\n",
    "    collate_fn=naive_collate,\n",
    "    drop_last=False,\n",
    ")\n",
    "#Create the model based on the task configuration file\n",
    "logger.info(\"Creating model...\")\n",
    "task = TrainingTask(cfg, evaluator)\n",
    "#Load the model weights if task is not 0\n",
    "if \"load_model\" in cfg.schedule:\n",
    "    ckpt = torch.load(cfg.schedule.load_model)\n",
    "    if \"pytorch-lightning_version\" not in ckpt:\n",
    "        warnings.warn(\n",
    "            \"Warning! Old .pth checkpoint is deprecated. \"\n",
    "            \"Convert the checkpoint with tools/convert_old_checkpoint.py \"\n",
    "        )\n",
    "        ckpt = convert_old_model(ckpt)\n",
    "    load_model_weight(task.model, ckpt, logger)\n",
    "    logger.info(\"Loaded model weight from {}\".format(cfg.schedule.load_model))\n",
    "model_resume_path = (\n",
    "    os.path.join(cfg.save_dir, \"model_last.ckpt\")\n",
    "    if \"resume\" in cfg.schedule\n",
    "    else None\n",
    ")\n",
    "#Set the device to GPU if available\n",
    "if cfg.device.gpu_ids == -1:\n",
    "    logger.info(\"Using CPU training\")\n",
    "    accelerator, devices, strategy, precision = (\n",
    "        \"cpu\",\n",
    "        None,\n",
    "        None,\n",
    "        cfg.device.precision,\n",
    "    )\n",
    "else:\n",
    "    accelerator, devices, strategy, precision = (\n",
    "        \"gpu\",\n",
    "        cfg.device.gpu_ids,\n",
    "        None,\n",
    "        cfg.device.precision,\n",
    "    )\n",
    "\n",
    "if devices and len(devices) > 1:\n",
    "    strategy = \"ddp\"\n",
    "    env_utils.set_multi_processing(distributed=True)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=cfg.save_dir,\n",
    "    max_epochs=cfg.schedule.total_epochs,\n",
    "    check_val_every_n_epoch=10,\n",
    "    accelerator=accelerator,\n",
    "    devices=[2],\n",
    "    log_every_n_steps=cfg.log.interval,\n",
    "    num_sanity_val_steps=0,\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=0)],\n",
    "    logger=logger,\n",
    "    benchmark=cfg.get(\"cudnn_benchmark\", True),\n",
    "    gradient_clip_val=cfg.get(\"grad_clip\", 0.0),\n",
    "    strategy=strategy,\n",
    "    precision=precision,\n",
    ")\n",
    "trainer.fit(task, train_dataloader, val_dataloader, ckpt_path=model_resume_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# PAPER PLOTS #\n",
    "###############\n",
    "\n",
    "# Assuming your method is the last one in the list\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import rc\n",
    "import matplotlib\n",
    "\n",
    "strategy = ['Fine-Tuning', 'Classic Replay', 'LwF', 'SID', 'Latent Distillation (ours)']\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "# Use different markers for different methods\n",
    "markers = ['o', 'o', 'o', 'o', 'o']\n",
    "colors = ['purple', 'blue', 'orange', 'green', 'red']\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "memory = [1.2, 1.2, 2.4, 2.4, 1.59]\n",
    "mAP = [3.2, 16.3, 21.7, 22.8, 23.6]\n",
    "\n",
    "# Parameter that determines the size of the dots\n",
    "size_parameter = [200, 200, 200, 200, 200]\n",
    "\n",
    "for i in range(len(strategy)):\n",
    "    plt.scatter(memory[i], mAP[i], marker=markers[i], color=colors[i], label=strategy[i], s=size_parameter[i], zorder=2)\n",
    "\n",
    "plt.grid(True, zorder=1)\n",
    "\n",
    "plt.xlabel('Total Parameters (Million)')\n",
    "plt.ylabel('Overall mAP@[0.50:0.95]')\n",
    "plt.xticks()\n",
    "plt.title('Total Parameters vs Overall mAP@[0.50:0.95] on scenario 19p1')\n",
    "plt.legend(title='CL strategy', loc='lower right')\n",
    "plt.savefig('mAPvsMem.eps', format='eps')\n",
    "plt.show()\n",
    "\n",
    "training_sets = ['base 15', '+ pottedplant', '+ sheep', '+ sofa', '+ train', '+ tvmonitor']\n",
    "strategy = ['Naive', 'Classic Replay', 'LwF', 'SID', 'Latent Distillation (ours)']\n",
    "colors = ['purple', 'blue', 'orange', 'green', 'red']\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import rc\n",
    "import matplotlib\n",
    "\n",
    "# Use LaTeX for text and set font size\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "auc_values = [[0.319, 0.07276294154642317, 0.017718051470568385, 0.007125799587737094, 0.00463388840764482, 0.0064597139891013665],\n",
    "              [0.319, 0.203, 0.176, 0.101, 0.077, 0.050],\n",
    "              [0.319, 0.206, 0.121, 0.028, 0.007, 0.003],\n",
    "              [0.319, 0.204, 0.144, 0.156, 0.104, 0.101],\n",
    "              [0.319, 0.228, 0.173, 0.148, 0.11, 0.096]]\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(0,5):\n",
    "    plt.plot(training_sets, auc_values[i], marker='o',color=colors[i], label=strategy[i])\n",
    "plt.xlabel('Task')\n",
    "plt.ylabel('mAP@[0.50:0.95]')\n",
    "plt.xticks(np.arange(len(training_sets)), training_sets)\n",
    "plt.title('Task stream on VOC2007 scenario 15p1')\n",
    "plt.legend(title='CL strategy', loc='upper right') #box_to_anchor=(1.05, 1),\n",
    "plt.grid(True)\n",
    "plt.savefig('mAP_15p1.eps', format='eps')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanodet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
