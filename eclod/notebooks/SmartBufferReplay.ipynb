{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T09:22:46.239761609Z",
     "start_time": "2024-02-09T09:22:44.559107778Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "import yaml\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "from pytorch_lightning.accelerators import find_usable_cuda_devices\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nanodet.data.collate import naive_collate\n",
    "from nanodet.data.dataset import build_dataset\n",
    "from nanodet.evaluator import build_evaluator\n",
    "from nanodet.trainer.task import TrainingTask\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from nanodet.util import (\n",
    "    NanoDetLightningLogger,\n",
    "    cfg,\n",
    "    convert_old_model,\n",
    "    env_utils,\n",
    "    load_config,\n",
    "    load_model_weight,\n",
    "    mkdir,\n",
    ")\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from nanodet.data.dataset.coco import CocoDataset\n",
    "\n",
    "#Set logger and seed\n",
    "logger = NanoDetLightningLogger('test')\n",
    "pl.seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T09:22:46.254664715Z",
     "start_time": "2024-02-09T09:22:46.247428808Z"
    }
   },
   "outputs": [],
   "source": [
    "#Classes to handle the datasets in the replay scenario\n",
    "\n",
    "class ReplayDataLoader:\n",
    "    def __init__(self, dataset1, dataset2, batch_size, shuffle):\n",
    "        \"\"\"\n",
    "        This class is used to create a dataloader that creates batches\n",
    "        using the task specific dataset and the replay buffer dataset.\n",
    "        Batches are created using 50% of the task specific dataset and 50% of the replay buffer dataset.\n",
    "        The iterators are reset when the task specific dataset is exhausted.\n",
    "        If the replay buffer dataset is exhausted before the task specific dataset, its iterator is reset.\n",
    "        \n",
    "        Args:\n",
    "            dataset1: task specific dataset\n",
    "            dataset2: replay buffer dataset\n",
    "            batch_size: Batch size\n",
    "            shuffle: Whether to shuffle the data\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dataset1 = dataset1\n",
    "        self.dataset2 = dataset2\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.dataset1_loader = torch.utils.data.DataLoader(\n",
    "            self.dataset1,\n",
    "            batch_size=self.batch_size//2,\n",
    "            shuffle=self.shuffle,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "            collate_fn=naive_collate,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        self.dataset2_loader = torch.utils.data.DataLoader(\n",
    "            self.dataset2,\n",
    "            batch_size=self.batch_size//2,\n",
    "            shuffle=self.shuffle,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "            collate_fn=naive_collate,\n",
    "            drop_last=True,\n",
    "        )\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.dataset1_iter = iter(self.dataset1_loader)\n",
    "        self.dataset2_iter = iter(self.dataset2_loader)\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        try:\n",
    "            batch1 = next(self.dataset1_iter)\n",
    "        except StopIteration:\n",
    "            raise StopIteration\n",
    "        try:\n",
    "            batch2 = next(self.dataset2_iter)\n",
    "        except StopIteration:\n",
    "            self.dataset2_iter = iter(self.dataset2_loader)\n",
    "            batch2 = next(self.dataset2_iter)\n",
    "            \n",
    "        merged_batch = {}\n",
    "        for key in batch1.keys():\n",
    "            if key == 'img':\n",
    "                merged_batch[key] = batch1[key] + batch2[key]\n",
    "            elif key == 'img_info':\n",
    "                merged_batch[key] = {k: batch1[key][k] + batch2[key][k] for k in batch1[key]}\n",
    "            elif key in ['gt_bboxes', 'gt_labels', 'gt_bboxes_ignore', 'warp_matrix']:\n",
    "                #merged_batch[key] = [torch.cat((torch.tensor(b1), torch.tensor(b2))) for b1, b2 in zip(batch1[key], batch2[key])]\n",
    "                merged_batch[key] = batch1[key] + batch2[key]\n",
    "            else:\n",
    "                raise ValueError(f\"Key not recognized\")\n",
    "        \n",
    "        return merged_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset1)\n",
    "        \n",
    "class SmartBufferDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This class is used to create a replay buffer dataset that stores random samples of the task specific dataset.\n",
    "    At init it takes random samples of the first task dataset to fill the buffer.\n",
    "    Then from task n to task n+1 it, where n>0, it updates 50% of the buffer with the new task dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset_n: task specific dataset\n",
    "        buffer_size: size of the replay buffer\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_n, buffer_size=250):\n",
    "        self.buffer_size = buffer_size\n",
    "        #At initialization, take random samples of the task 0 dataset to fill the buffer\n",
    "        self.buffer_indices = random.sample(range(len(dataset_n)), self.buffer_size)\n",
    "        self.buffer_dataset = Subset(dataset_n, self.buffer_indices)\n",
    "\n",
    "    def __getitem__(self, buff_index):\n",
    "        #Just return a buffer item at the index\n",
    "        return self.buffer_dataset[buff_index]\n",
    "\n",
    "    def __len__(self):\n",
    "        #Return the buffer size\n",
    "        return self.buffer_size\n",
    "\n",
    "    def update_buffer(self, dataset_np1, cfg):\n",
    "        #Smart update, based on the number of classes already contained in the buffer and the ones that are being added to the dataset\n",
    "        #E.g. Update only 1/16 of the buffer if only 1 new class is added to the dataset and the buffer contains 15 classes\n",
    "        new_classes = len(cfg.data.train.class_names)\n",
    "        val_classes = len(cfg.data.val.class_names)\n",
    "        ideal_samples_per_class = int(self.buffer_size/val_classes)\n",
    "        update_buffer_indices_n = random.sample(range(self.buffer_size), self.buffer_size - int(ideal_samples_per_class*new_classes))\n",
    "        update_buffer_indices_np1 = random.sample(range(len(dataset_np1)), int(ideal_samples_per_class*new_classes))\n",
    "        subset_n = Subset(self.buffer_dataset, update_buffer_indices_n)\n",
    "        subset_np1 = Subset(dataset_np1, update_buffer_indices_np1)\n",
    "        self.buffer_dataset = torch.utils.data.ConcatDataset([subset_n, subset_np1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T09:22:46.572136188Z",
     "start_time": "2024-02-09T09:22:46.567777805Z"
    }
   },
   "outputs": [],
   "source": [
    "#Function to create the task configuration file required for training\n",
    "def create_exp_cfg(yml_path, task):\n",
    "    all_names = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "    #Load the YAML file\n",
    "    with open(yml_path, 'r') as file:\n",
    "        temp_cfg = yaml.safe_load(file)\n",
    "    #Save dir of the model\n",
    "    temp_cfg['save_dir'] = 'models/task' + str(task)\n",
    "    #If base task, training and testing classes are the same\n",
    "    if task == 0:\n",
    "        temp_cfg['data']['train']['class_names'] = all_names[:15]\n",
    "        temp_cfg['data']['val']['class_names'] = all_names[:15]\n",
    "        temp_cfg['model']['arch']['head']['num_classes'] = 20 #15\n",
    "        #temp_cfg['model']['arch']['aux_head']['num_classes'] = 20 #15\n",
    "    #Else, training only on task specific class, and testing on all classes\n",
    "    else:\n",
    "        temp_cfg['data']['train']['class_names'] = [all_names[14+task]]\n",
    "        temp_cfg['data']['val']['class_names'] = all_names[:15+task]\n",
    "        temp_cfg['model']['arch']['head']['num_classes'] = 20#15+task\n",
    "        #temp_cfg['model']['arch']['aux_head']['num_classes'] = 20#15+task\n",
    "        temp_cfg['schedule']['load_model'] = 'models/task' + str(task-1) + '/model_last.ckpt'\n",
    "        \n",
    "    temp_cfg_name = 'cfg/task' + str(task) + '.yml'\n",
    "    print(temp_cfg_name)\n",
    "    #Save the new configuration file\n",
    "    with open(temp_cfg_name, 'w') as file:\n",
    "        yaml.safe_dump(temp_cfg, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T09:22:47.836652884Z",
     "start_time": "2024-02-09T09:22:47.494268949Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####TEMP INITIALIZATION since the base model is already available\n",
    "task = 0\n",
    "create_exp_cfg('cfg/VOC.yml', task)\n",
    "load_config(cfg, 'cfg/task' + str(task) + '.yml')\n",
    "#Build datasets and dataloaders based on the task configuration file\n",
    "train_dataset = build_dataset(cfg.data.train, \"train\")\n",
    "#val_dataset = build_dataset(cfg.data.val, \"test\")\n",
    "buffer_dataset = SmartBufferDataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T09:55:59.489270411Z",
     "start_time": "2024-02-09T09:22:49.286992344Z"
    }
   },
   "outputs": [],
   "source": [
    "###LEARNING STREAM###\n",
    "#task 0: train on first 15 classes, test on 15 classes\n",
    "#task 1: train on class n°16, test on 16 classes\n",
    "#task 2: train on class n°17, test on 17 classes\n",
    "#task 3: train on class n°18, test on 18 classes\n",
    "#task 4: train on class n°19, test on 19 classes\n",
    "#task 5: train on class n°20, test on 20 classes\n",
    "for task in range (1, 6):\n",
    "    logger = NanoDetLightningLogger('run_logs/task'+str(task))\n",
    "    logger.info(\"Starting task\" + str(task))\n",
    "    logger.info(\"Setting up data...\")\n",
    "    #Create the task configuration file based on the task number and load the configuration\n",
    "    create_exp_cfg('cfg/VOC.yml', task)\n",
    "    load_config(cfg, 'cfg/task' + str(task) + '.yml')\n",
    "    #Build datasets and dataloaders based on the task configuration file\n",
    "    train_dataset = build_dataset(cfg.data.train, \"train\")\n",
    "    #If task is not 0, create the replay dataset using the buffer\n",
    "    if task == 0:\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=cfg.device.batchsize_per_gpu,\n",
    "            shuffle=True if task == 0 else False, #Shuffling is done inside ReplayDataset class\n",
    "            num_workers=cfg.device.workers_per_gpu,\n",
    "            pin_memory=True,\n",
    "            collate_fn=naive_collate,\n",
    "            drop_last=True,\n",
    "        )\n",
    "    else:\n",
    "        train_dataloader = ReplayDataLoader(\n",
    "            train_dataset, \n",
    "            buffer_dataset, \n",
    "            cfg.device.batchsize_per_gpu,\n",
    "            shuffle = True\n",
    "        )\n",
    "\n",
    "    val_dataset = build_dataset(cfg.data.val, \"test\")\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=cfg.device.batchsize_per_gpu,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.device.workers_per_gpu,\n",
    "        pin_memory=True,\n",
    "        collate_fn=naive_collate,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    evaluator = build_evaluator(cfg.evaluator, val_dataset)\n",
    "    \n",
    "    #Create the model based on the task configuration file\n",
    "    logger.info(\"Creating model...\")\n",
    "    TrainTask = TrainingTask(cfg, evaluator)\n",
    "    #Load the model weights if task is not 0\n",
    "    if \"load_model\" in cfg.schedule:\n",
    "        ckpt = torch.load(cfg.schedule.load_model)\n",
    "        if \"pytorch-lightning_version\" not in ckpt:\n",
    "            warnings.warn(\n",
    "                \"Warning! Old .pth checkpoint is deprecated. \"\n",
    "                \"Convert the checkpoint with tools/convert_old_checkpoint.py \"\n",
    "            )\n",
    "            ckpt = convert_old_model(ckpt)\n",
    "        load_model_weight(TrainTask.model, ckpt, logger)\n",
    "        logger.info(\"Loaded model weight from {}\".format(cfg.schedule.load_model))\n",
    "    model_resume_path = (\n",
    "        os.path.join(cfg.save_dir, \"model_last.ckpt\")\n",
    "        if \"resume\" in cfg.schedule\n",
    "        else None\n",
    "    )\n",
    "    #Set the device to GPU if available\n",
    "    if cfg.device.gpu_ids == -1:\n",
    "        logger.info(\"Using CPU training\")\n",
    "        accelerator, devices, strategy, precision = (\n",
    "            \"cpu\",\n",
    "            None,\n",
    "            None,\n",
    "            cfg.device.precision,\n",
    "        )\n",
    "    else:\n",
    "        accelerator, devices, strategy, precision = (\n",
    "            \"gpu\",\n",
    "            cfg.device.gpu_ids,\n",
    "            None,\n",
    "            cfg.device.precision,\n",
    "        )\n",
    "\n",
    "    if devices and len(devices) > 1:\n",
    "        strategy = \"ddp\"\n",
    "        env_utils.set_multi_processing(distributed=True)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=cfg.save_dir,\n",
    "        max_epochs=cfg.schedule.total_epochs,\n",
    "        check_val_every_n_epoch=cfg.schedule.val_intervals,\n",
    "        accelerator=accelerator,\n",
    "        devices=[2],\n",
    "        log_every_n_steps=cfg.log.interval,\n",
    "        num_sanity_val_steps=0,\n",
    "        callbacks=[TQDMProgressBar(refresh_rate=0)],\n",
    "        logger=logger,\n",
    "        benchmark=cfg.get(\"cudnn_benchmark\", True),\n",
    "        gradient_clip_val=cfg.get(\"grad_clip\", 0.0),\n",
    "        strategy=strategy,\n",
    "        precision=precision,\n",
    "    )\n",
    "    trainer.fit(TrainTask, train_dataloader, val_dataloader, ckpt_path=model_resume_path)\n",
    "    \n",
    "    #Replay code\n",
    "    #If task is 0, initialize the replay buffer with the task 0 dataset \n",
    "    #if task > 0 update the buffer with the new task dataset\n",
    "    if task == 0:\n",
    "        print(\"Creating buffer dataset\")\n",
    "        buffer_dataset = SmartBufferDataset(train_dataset)\n",
    "    else:\n",
    "        print(\"Updating buffer dataset\")\n",
    "        buffer_dataset.update_buffer(train_dataset, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# TESTING\n",
    "\n",
    "To get testing results, use this code in terminal:\n",
    "```bash\n",
    "python test.py --task val --config /home/pasti/PycharmProjects/nanodet_cl/eclod/cfg/Replay/task2.yml --model /home/pasti/PycharmProjects/nanodet_cl/eclod/models/Replay/task2/model_last.ckpt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanodet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
